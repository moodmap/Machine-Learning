#My first delve into coding an end to end image classifier, using the 'hello world' data set of hand written digits
#Following along with a datacamp.com tutorial for this one. Although I play a lot with the code it is not all original and from my own mind.
#Importing libraries and the data
from sklearn import datasets
digits = datasets.load_digits()
import numpy as np

#Seeing what the data is composed of
print(digits.keys())
print(digits.data)
print(digits.target)
print(digits.DESCR)

#The shape shows that we have 1797 training examples. Each of these have 64 elements to them (each image is 8x8 pixels)
digits_data = digits.data
print(digits_data.shape)
digits_target = digits.target
print(digits_target.shape)

#The length is 10 which makes sense - all the digits from 0-9
number_digits = len(np.unique(digits.target))
print(number_digits)
digits_images = digits.images
print(digits_images.shape)

#Compacting the 8,8 into a single dimension of 64
digits.images.reshape((1797, 64))

#taking a look at the images
import matplotlib.pyplot as plt
fig = plt.figure(figsize=(6,6))
fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)
for i in range(64):
    ax = fig.add_subplot(8,8,i+1,xticks=[],yticks=[])
    ax.imshow(digits.images[i],cmap=plt.cm.binary,interpolation='nearest')
    ax.text(0,7,str(digits.target[i]))
plt.show()

#Doing some dimensionality reduction - using a linear transformation method
#that gives back to us the principal components that maximise the variance
#of the data
from sklearn.decomposition import RandomizedPCA
from sklearn.decomposition import PCA
# Create a Randomized PCA model that takes two components
randomized_pca = RandomizedPCA(n_components=2)

# Fit and transform the data to the model
reduced_data_rpca = randomized_pca.fit_transform(digits.data)

# Create a regular PCA model 
pca = PCA(n_components=2)

# Fit and transform the data to the model
reduced_data_pca = pca.fit_transform(digits.data)

# Inspect the shape
reduced_data_pca.shape

# Print out the data
print(reduced_data_rpca)
#I believe that PCA does better. According to the tutorial RPCA works better on 
#Data with higher dimenionality.

print(reduced_data_pca)
colors = ['black','blue','purple','yellow','white','red','lime','cyan','orange','gray']
for i in range(len(colors)):
    x = reduced_data_rpca[:, 0][digits.target == i]
    y = reduced_data_rpca[:, 1][digits.target == i]
    plt.scatter(x,y,c=colors[i])
plt.legend(digits.target_names,bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.title("PCA Scatter Plot")
plt.show()

#Preprocessing the data, standardizing the digits data
from sklearn.preprocessing import scale
data = scale(digits.data)

#Splitting up the data
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test, images_train, images_test = train_test_split(data, digits.target, digits.images, test_size=0.25, random_state=42)
n_samples, n_features = X_train.shape

#Number of training features
print(n_samples)
#Number of features (8x8)
print(n_features)
n_digits = len(np.unique(y_train))
print(len(y_train))

#Running KMeans clustering, with 10 clusters, hoping each cluster will
#Centre around each potential digit
from sklearn import cluster
clf = cluster.KMeans(init='k-means++',n_clusters=10,random_state=42)
clf.fit(X_train)

#Looks like some of the digits are repeated here so obviously the 
#Clustering isn't working as well as I'd like
import matplotlib.pyplot as plt
fig = plt.figure(figsize=(8,3))
fig.suptitle('Cluster Center Images', fontsize=14, fontweight='bold')

for i in range(10):
    ax = fig.add_subplot(2,5,1+i)
    ax.imshow(clf.cluster_centers_[i].reshape((8,8)), cmap=plt.cm.binary)
    plt.axis('off')
plt.show()

# Predict the labels for `X_test`
y_pred=clf.predict(X_test)

# Print out the first 100 instances of `y_pred`
print(y_pred[:100])

# Print out the first 100 instances of `y_test`
print(y_test[:100])

# Study the shape of the cluster centers
clf.cluster_centers_.shape

#Running PCA and comparing train with test. Clustering looks kind of similar..
from sklearn.decomposition import PCA
X_pca = PCA(n_components=2).fit_transform(X_train)
clusters=clf.fit_predict(X_train)
fig,ax=plt.subplots(1,2,figsize=(8,4))

fig.suptitle('Predicted Versus Training Labels',fontsize=14,fontweight='bold')
fig.subplots_adjust(top=0.85)

ax[0].scatter(X_pca[:, 0], X_pca[:, 1], c=clusters)
ax[0].set_title('Predicted Training Labels')
ax[1].scatter(X_pca[:, 0], X_pca[:, 1], c=y_train)
ax[1].set_title('Actual Training Labels')
plt.show()

#Running some metrics. The confusion matrix I understand but the others
#Not as much yet. Will have to study. One thing I do know is that they reflect
#That the algorithm didn't work too well on fitting the training data
from sklearn import metrics
print(metrics.confusion_matrix(y_test,y_pred))

from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score, adjusted_mutual_info_score, silhouette_score
print('% 9s' % 'inertia    homo   compl  v-meas     ARI AMI  silhouette')
print('%i   %.3f   %.3f   %.3f   %.3f   %.3f    %.3f'
          %(clf.inertia_,
      homogeneity_score(y_test, y_pred),
      completeness_score(y_test, y_pred),
      v_measure_score(y_test, y_pred),
      adjusted_rand_score(y_test, y_pred),
      adjusted_mutual_info_score(y_test, y_pred),
      silhouette_score(X_test, y_pred, metric='euclidean')))
      
#Trying SVM, support vector machine
# Import `train_test_split`
from sklearn.cross_validation import train_test_split

# Split the data into training and test sets 
X_train, X_test, y_train, y_test, images_train, images_test = train_test_split(digits.data, digits.target, digits.images, test_size=0.25, random_state=42)

# Import the `svm` model
from sklearn import svm

# Create the SVC model 
svc_model = svm.SVC(gamma=0.001, C=100., kernel='linear')

# Fit the data to the SVC model
svc_model.fit(X_train, y_train)

# Import `train_test_split`
from sklearn.cross_validation import train_test_split

# Split the data into training and test sets 
X_train, X_test, y_train, y_test, images_train, images_test = train_test_split(digits.data, digits.target, digits.images, test_size=0.25, random_state=42)

# Import the `svm` model
from sklearn import svm

# Create the SVC model 
svc_model = svm.SVC(gamma=0.001, C=100., kernel='linear')

# Fit the data to the SVC model
svc_model.fit(X_train, y_train)

# Split the `digits` data into two equal sets
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.5, random_state=0)

# Import GridSearchCV
from sklearn.grid_search import GridSearchCV

# Set the parameter candidates
parameter_candidates = [
  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},
  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},
]

# Create a classifier with the parameter candidates
clf = GridSearchCV(estimator=svm.SVC(), param_grid=parameter_candidates, n_jobs=-1)

# Train the classifier on training data
clf.fit(X_train, y_train)

# Print out the results 
print('Best score for training data:', clf.best_score_)
print('Best `C`:',clf.best_estimator_.C)
print('Best kernel:',clf.best_estimator_.kernel)
print('Best `gamma`:',clf.best_estimator_.gamma)

# Apply the classifier to the test data, and view the accuracy score
clf.score(X_test, y_test)  

# Train and score a new classifier with the grid search parameters
svm.SVC(C=10, kernel='rbf', gamma=0.001).fit(X_train, y_train).score(X_test, y_test)
#99%! Not bad. Making a note of the success using SVM in this context!

print(svc_model.predict(X_test))
print(y_test)

# Assign the predicted values to `predicted`
predicted = svc_model.predict(X_test)

# Zip together the `images_test` and `predicted` values in `images_and_predictions`
images_and_predictions = list(zip(images_test, predicted))

# For the first 4 elements in `images_and_predictions`
for index, (image, prediction) in enumerate(images_and_predictions[:4]):
    # Initialize subplots in a grid of 1 by 4 at positions i+1
    plt.subplot(1, 4, index + 1)
    # Don't show axes
    plt.axis('off')
    # Display images in all subplots in the grid
    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
    # Add a title to the plot
    plt.title('Predicted: ' + str(prediction))

# Show the plot
plt.show()

#OK I'm confused..the four predictions above are all incorrect, but it's saying
#We've got an accuracy of 99%...somethings' up
# Import `metrics`
from sklearn import metrics

# Print the classification report of `y_test` and `predicted`
print(metrics.classification_report(y_test,predicted))

# Print the confusion matrix of `y_test` and `predicted`
print(metrics.confusion_matrix(y_test,predicted))

             precision    recall  f1-score   support

          0       1.00      1.00      1.00        89
          1       0.99      1.00      0.99        90
          2       0.99      1.00      0.99        92
          3       0.99      0.97      0.98        93
          4       1.00      0.99      0.99        76
          5       0.98      1.00      0.99       108
          6       1.00      1.00      1.00        89
          7       1.00      1.00      1.00        78
          8       1.00      0.99      0.99        92
          9       0.99      0.99      0.99        92

avg / total       0.99      0.99      0.99       899

[[ 89   0   0   0   0   0   0   0   0   0]
 [  0  90   0   0   0   0   0   0   0   0]
 [  0   0  92   0   0   0   0   0   0   0]
 [  0   0   1  90   0   1   0   0   0   1]
 [  0   1   0   0  75   0   0   0   0   0]
 [  0   0   0   0   0 108   0   0   0   0]
 [  0   0   0   0   0   0  89   0   0   0]
 [  0   0   0   0   0   0   0  78   0   0]
 [  0   0   0   0   0   1   0   0  91   0]
 [  0   0   0   1   0   0   0   0   0  91]]

# Import `Isomap()`
from sklearn.manifold import Isomap

# Create an isomap and fit the `digits` data to it
X_iso = Isomap(n_neighbors=10).fit_transform(X_train)

# Compute cluster centers and predict cluster index for each sample
predicted = svc_model.predict(X_train)

# Create a plot with subplots in a grid of 1X2
fig, ax = plt.subplots(1, 2, figsize=(8, 4))

# Adjust the layout
fig.subplots_adjust(top=0.85)

# Add scatterplots to the subplots 
ax[0].scatter(X_iso[:, 0], X_iso[:, 1], c=predicted)
ax[0].set_title('Predicted labels')
ax[1].scatter(X_iso[:, 0], X_iso[:, 1], c=y_train)
ax[1].set_title('Actual Labels')


# Add title
fig.suptitle('Predicted versus actual labels', fontsize=14, fontweight='bold')

# Show the plot
plt.show()

#Looking very similar indeed on the plots!
