#A Practise exercise for KNN from a Udemy bootcamp course on data science and machine learning
#Simply KNN classifies data points with labelled data most closely associated with them, based on a n_numbers value
#Selecting n_numbers based on which returns a minimal error rate, using a for loop to find out.

#Importing the relevant libraries
import pandas as pd
import numpy as np
import sklearn
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline

#Opening file, seeing data, quickly visualising
df = pd.read_csv("KNN_Project_Data")
df.head()
sns.pairplot(df,hue='TARGET CLASS',palette='coolwarm')

#Standardising the variables
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(df)
scaler.fit(df.drop('TARGET CLASS',axis=1))
scaled_features = scaler.transform(df.drop('TARGET CLASS',axis=1))

#Splitting data into train and test
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(scaled_features, df['TARGET CLASS'], test_size=0.3)

#Fitting the training data to the KNN algorithm, starting with a low n_neighbors value
from sklearn.neighbors import KNeighborsClassifier
KNN = KNeighborsClassifier(n_neighbors = 1)
KNN.fit(X_train,y_train)

#Creating an initial prediction with the test data and comparing the the y_test
predictions = KNN.predict(X_test)
from sklearn.metrics import classification_report,confusion_matrix
print(confusion_matrix(y_test, predictions))
print(classification_report(y_test, predictions))

precision    recall  f1-score   support

          0       0.75      0.78      0.76       148
          1       0.78      0.75      0.76       152

avg / total       0.76      0.76      0.76       300

#Using a for loop and graph of the error rates to decide on the most appropriate n_neighbors value
error_rate = []
for i in range(1,40):
    knn = KNeighborsClassifier(n_neighbors = i)
    knn.fit(X_train, y_train)
    pred_i = knn.predict(X_test)
    error_rate.append(np.mean(pred_i != y_test))

plt.figure(figsize=(10,6))
plt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')

#Selected 30, as this had the lowest error
knn = KNeighborsClassifier(n_neighbors = 30)

#Testing KNN out with 30 this time
knn.fit(X_train, y_train)
predictions = knn.predict(X_test)

print('WITH K=30')
print('\n')
print(confusion_matrix(y_test,predictions))
print('\n')
print(classification_report(y_test,predictions))


             precision    recall  f1-score   support

          0       0.78      0.81      0.80       148
          1       0.81      0.78      0.80       152

avg / total       0.80      0.80      0.80       300


#Average precision and recall both improved by 0.4
