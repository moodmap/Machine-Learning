#Practising opening data, understanding and visualising it, splitting into training and test sets, and doing some basic linear regression

#importing the required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
from sklearn.model_selection import train_test_split

#Getting a quick feel for the data
df = pd.read_csv("USA_Housing.csv")
df.head()
df.info()
df.describe()
df.columns

#Visualising the data
sns.pairplot(df)
sns.distplot(df['Price'])
sns.heatmap(df.corr(),annot=True)

#Separating the data into x and y and splitting into training and test sets
df.columns
X = df[['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms',
       'Avg. Area Number of Bedrooms', 'Area Population']]
y = df['Price']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)

#Some basic linear regression to practise what I've learned today
lm = LinearRegression()
lm.fit(X_train, y_train)
print(lm.intercept_)
lm.coef_
cdf = pd.DataFrame(lm.coef_,X.columns,columns=['Coeff'])
cdf

#Making predictions on the test set
predictions = lm.predict(X_test)
predictions
y_test

#visualising how effective the linear regression algorithm has been on our test set
#Scatter plot looks highly correlated between yi and y
plt.scatter(y_test,predictions)

#distribution of residuals - seems to be normally distributed around Â£0 so looks like an effective model
sns.distplot((y_test-predictions)) 

#Getting a value on the loss of the function
from sklearn import metrics

#MAE - easy to use
metrics.mean_absolute_error(y_test,predictions)

#MSE - more effective in real life, as it 'punishes' high error points.
metrics.mean_squared_error(y_test, predictions)

#RMSE - most popular to use
np.sqrt(metrics.mean_squared_error(y_test, predictions))

